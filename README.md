# Stochastic GFlowNets for Kidney Exchange Programs: A Critical Extension

## Overview

By integrating Stochastic GFlowNets ([Pan et al. (2023)](https://arxiv.org/abs/2302.09465)), we address inherent limitations in deterministic flow models under Markovian transitions in KEP, demonstrating through mathematical dissection how unmodeled stochasticity and time-dependency bias marginals, fairness metrics, and long-term expectations. Specifically, treating multi-round dynamics as independent episodes ignores the feedback loop where policy choices in early rounds (e.g., round 1 matchings generated by the GFlowNet) directly impact the probability distribution of later states (e.g., graphs in round 20), rendering the process a single interdependent "mega-trajectory" rather than isolated rounds. This unmodeled dependency distorts the training loss, as the likelihood of certain graphs contributing to the objective isn't properly conditioned on the policy's history. The goal is to foster rigorous probabilistic fidelity in high-stakes combinatorial optimization, where approximations must confront the full entropy of real-world dynamics.

## Purpose and Contributions

- **Mathematical Analysis**: We formalize the divergence arising from deterministic assumptions in stochastic environments, proving non-zero Kullback-Leibler gaps that inflate variance and collapse modes.
- **Simulation Framework**: Conceptual tools to replicate biases in toy KEPs, highlighting cascades in expected transplants and pool composition.
- **Ethical Imperative**: In KEPs, where each transplant equates to substantial quality-adjusted life years (valued at >$1.5M per case in health economics), unaddressed biases represent a quantifiable inefficiency. This repo advocates for stochastic integration to align models with probabilistic reality.

No executable code is included; simulations are conceptual and can be implemented via standard libraries (e.g., NumPy for Monte-Carlo runs). For replication, refer to the mathematical sections below.

## Installation and Usage

This is a documentation-only repository. Clone for reference:

```
git clone https://github.com/yourusername/stochastic-kep-critique.git
```

No dependencies required beyond a LaTeX viewer for equations. Explore the mathematical dissection for insights; extend via cited works.

## Mathematical Dissection: Biases in Deterministic GFlowNets for Stochastic KEPs from a Social Planner's Perspective

In the context of [St-Arnaud et al. (2025)](https://openreview.net/pdf?id=IizmQoF86Y), GFlowNets are employed to learn a distribution over optimal or near-optimal matchings in kidney exchange programs (KEPs), where the compatibility graph $G_t = (V_t, E_t)$ at round $t$ represents patient-donor pairs as vertices and compatible donation opportunities as directed edges. The goal is to sample matchings $H_t \subseteq E_t$ (cycles and chains of bounded length, typically ≤3) proportional to a reward function $R(H_t)$, which may encode the number of transplants, weighted by equity considerations or QALY gains (estimated at ~$1.5M per transplant per [McCormick et al. (2022)](https://www.sciencedirect.com/science/article/pii/S109830152201957X)). However, from a social planner's perspective, the true objective is to select a policy $\pi$ that maximizes the discounted expected welfare over a horizon:

<p align="center">
  <img src="https://latex.codecogs.com/png.latex?V(\pi)%20=%20\mathbb{E}\!\left[\sum_{t=1}^{\infty}\gamma^{t-1}R(H_t)\,\middle|\,H_t\sim\pi(s_t),\,s_{t+1}\sim%20M(s_t,H_t)\right]">
</p>

where $s_t = G_t$ is the state, $M$ is the stochastic transition kernel capturing graph evolution (arrivals, departures, and matching removals), and $\gamma \in [0,1)$ discounts future rounds to reflect urgency and uncertainty in patient outcomes. This formulation treats the KEP as a Markov Decision Process (MDP), where actions are matchings $H_t$, and the planner seeks to balance immediate transplants with long-term pool sustainability, avoiding accumulation of hard-to-match patients (e.g., high-cPRA or O-blood-type recipients).

Quick note: Already, we notice that the rounds are time-dependent because matchings today influence matchings tomorrow (e.g., removing matched pairs alters future compatibility edges and pool composition). This will also influence the probability that a specific episode (trajectory $\vartheta_i$, encoding a sequence of edge selections leading to a matching) is included in the loss during training; let's name this probability $P(\vartheta_i \in \mathcal{L})$, where $\mathcal{L}$ is the loss term. To illustrate, a particular episode has much more chance to arrive in a low round than a high round— in early rounds, graphs are smaller with fewer vertices and edges, leading to a sparser action space and higher likelihood of sampling any given $\vartheta_i$ (e.g., via uniform or reward-proportional exploration); in later rounds, larger graphs explode the trajectory space, diluting $P(\vartheta_i \in \mathcal{L})$ for any fixed $\vartheta_i$, potentially biasing training toward low-round dynamics unless corrected by importance sampling.

The authors' simulator, adapted from [Saidman et al. (2006)](https://pubmed.ncbi.nlm.nih.gov/16534482/), faithfully models stochastic graph evolution: incompatible pairs arrive via Poisson process with rate $\lambda = 5$ per round, blood types sampled from empirical distributions (O: 48.14%, A: 33.73%, B: 14.28%, AB: 3.85%), compatibility edges added per ABO rules ([Dean, 2005](https://www.ncbi.nlm.nih.gov/books/NBK2261/)), and outgoing edges stochastically removed via Bernoulli trials parameterized by calculated panel reactive antibody (cPRA) levels (low: 0.05 with 70.19% probability, medium: 0.45 with 20%, high: 0.90 with 9.81%; Tinckam et al., 2015). Graphs are constructed over $N=20$ rounds, yielding expected 100 vertices, but episodes focus on single-round dynamics ($N=1$) with 1000 trajectories per graph, generating a dataset of 100,000 vectors $(\vartheta_1, \dots, \vartheta_L)$, where each $\vartheta_i$ encodes a trajectory. Initial graph embeddings condition the model, with architecture details in their Table 6.

However, this setup treats the 20 rounds as sources of independent graphs for single-round training, overlooking the endogenous dependency introduced by the GFlowNet policy itself. In deployment, the policy's stochastic generation of H_1 in round 1 directly influences P(G_2 | G_1, H_1), and cascades to P(G_{20} | ·), making the entire horizon a single "mega-trajectory" rather than 20 isolated episodes. During training, the dataset's graphs are generated under an implicit baseline policy (e.g., during simulation), so the probability of a specific G_t contributing to the TB loss isn't modeled as policy-dependent—P(G_t | π)—leading to biased marginals. This mismatch falsifies the loss, as rare or policy-induced graphs (e.g., those preserving diversity) are under-represented, amplifying short-sighted greediness.

Standard GFlowNets train to sample matchings $H$ proportional to $R(H)$, enforcing trajectory balance (TB):

<p align="center">
  <img src="https://latex.codecogs.com/png.latex?Z%28s_0%3B%20%5Cphi%29%20%5Cprod_%7Bi%3D1%7D%5En%20P_F%28s_i%20%5Cmid%20s_%7Bi-1%7D%3B%20%5Ctheta%29%20%3D%20R%28x%29%20%5Cprod_%7Bi%3D1%7D%5En%20P_B%28s_%7Bi-1%7D%20%5Cmid%20s_i%3B%20%5Ctheta%29%2C" alt="Trajectory Balance Equation">
</p>

where $s_0$ is the initial state (empty matching on $G$), $\tau = (s_0 \to \dots \to s_n = x)$ is a trajectory terminating at maximal matching $x = (G, H)$, $Z$ is the learned partition function, $P_F$ and $P_B$ are forward and backward policies, and marginal termination probabilities satisfy $P_T(x) \propto R(x)$. This implies detailed balance (DB) localization:

<p align="center">
  <img src="https://latex.codecogs.com/png.latex?F%28s%29%20P_F%28s%27%20%5Cmid%20s%29%20%3D%20F%28s%27%29%20P_B%28s%20%5Cmid%20s%27%29%2C" alt="Detailed Balance Equation">
</p>

with state flows $F(s) = \sum_{\tau \ni s} F(\tau)$.

However, KEPs exhibit stochastic transitions: after selecting $H$ in $s_t = (G_t, H_t)$, the next graph $G_{t+1} \sim M(G_t \setminus H_t)$, incorporating Poisson arrivals, sampled compatibilities, and cPRA-driven removals. This yields a non-degenerate kernel $P(G_{t+1} \mid G_t, H_t) > 0$ for multiple $G_{t+1}$, with entropy $\mathcal{H}[P(\cdot \mid G_t, H_t)] > 0$. The authors' GFlowNet assumes deterministic transitions $T(s_t, H_t) = s_{t+1}$, optimizing toward $\prod_t R(H_t)$ while ignoring the kernel, effectively treating rounds as independent or graphs as static.

This induces bias in the learned policy $  \pi_\theta(H \mid G) \approx P_F  $, preventing convergence to the true target distribution due to unmodeled stochastic transitions and time-dependencies. The true target posterior over multi-round trajectories $  \tau = (G_0, H_1, G_1, H_2, \dots, G_T, H_T)  $ is:

<p align="center">
  <img src="https://latex.codecogs.com/png.latex?P%5E%2A%28%5Ctau%29%20%5Cpropto%20%5Cprod_%7Bt%3D1%7D%5ET%20R%28H_t%29%20%5Ccdot%20P%28G_t%20%5Cmid%20G_%7Bt-1%7D%2C%20H_%7Bt-1%7D%29%2C" alt="Trajectory Posterior Equation">
</p>

with $P(\cdot \mid \cdot, \cdot)$ from $M$. The GFlowNet marginal $P_T(\tau) \propto \prod_t R(H_t)$ omits the kernel, yielding Kullback-Leibler divergence:

<p align="center">
  <img src="https://latex.codecogs.com/png.latex?D_%7B%5Cmathrm%7BKL%7D%7D%5Cleft%28P%5E%2A%20%5Cparallel%20P_T%5Cright%29%20%3D%20%5Cmathbb%7BE%7D_%7BP%5E%2A%7D%5Cleft%5B%5Csum_t%20%5Clog%20P%28G_t%20%5Cmid%20G_%7Bt-1%7D%2C%20H_%7Bt-1%7D%29%20%5Cright%5D%20-%20%5Cmathcal%7BH%7D%5BP%5E%2A%5D%20%2B%20%5Cmathcal%7BH%7D%5BP_T%5D" alt="KL divergence">
</p>

Since:

<p align="center">
  <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BH%7D%5BP%5E%2A%5D%20%3D%20%5Cmathcal%7BH%7D%5BP_T%5D%20%2B%20%5Csum_t%20%5Cmathbb%7BE%7D_%7BP%5E%2A%7D%20%5B%5Cmathcal%7BH%7D%5BP%28G_t%20%5Cmid%20%5Ccdot%29%5D%5D" alt="Entropy Decomposition Equation">
</p>

(by chain rule, with extra entropy from transitions), and the expectation term is positive unless $M$ is deterministic (impossible given Poisson/Bernoulli variability), **$D_{\text{KL}} > 0$**. This gap scales with branching: higher $\mathcal{H}[P(\cdot \mid \cdot)]$ biases $P_T$ toward low-variance modes, as unmodeled stochasticity penalizes exploratory paths with dispersed downstream rewards. In economic terms, this collapses to greedy, short-sighted policies, underweighting matchings that preserve pool diversity for future welfare gains—e.g., sparing hard-to-match pairs yields $\mathbb{E}[\sum R_t \mid H]$ inflated by replenishment, but variance suppresses sampling.

Gradient variance in TB exacerbates inefficiency:

<p align="center">
  <img src="https://latex.codecogs.com/png.latex?%5Cnabla%20L_%7B%5Ctext%7BTB%7D%7D%20%5Capprox%20%5Csum_%5Ctau%20%5Cnabla%20%5Clog%20P_F%28%5Ctau%29%20%5Ccdot%20%5Cleft%28%20%5Clog%20Z%20%2B%20%5Csum%20%5Clog%20P_F%20-%20%5Clog%20R%20-%20%5Csum%20%5Clog%20P_B%20%5Cright%29%2C" alt="Gradient of TB Loss">
</p>

where unmodeled $M$ injects noise via sampled $G_{t+1}$, with $\text{Var}[\nabla L] = O(T \cdot \bar{b})$ ($T$ rounds, $\bar{b}$ average branches). As $T \to \infty$, variance diverges, inducing mode collapse (Pan et al., 2023, Figs. 5-9), flattening $P_T$ to uniform under exploration ($\alpha \to 1$) or greedy under exploitation.

Time-dependency amplifies this on real data, fracturing the assumed static DAG. In the simulator, graphs accumulate stochasticity over 20 rounds, but single-round episodes ($N=1$) treat states as independent, ignoring propagation: later-round sparsity/edges depend on prior $H_t$ via $M$. Real KEPs exhibit stronger dependency due to demographic biases in cPRA: Black patients are disproportionately sensitized (cPRA >80% in ~25-30% of cases vs. ~15-20% for Whites; Reese et al., 2016, from UNOS data implying higher from disparities in prior exposures), as are Hispanics (~20-25% high cPRA; Pando et al., 2018). Statistically, Black candidates comprise ~32% of the waitlist but receive ~25% of transplants, with higher cPRA causing longer dwell times (OPTN/SRTR 2022 Report: Blacks have 2x ESRD incidence, lower transplant rates). This skews pool composition: minorities accumulate, altering edge densities (fewer outgoing for high-cPRA), making $P(G_t \mid \cdot)$ history-dependent. Proved via conditional entropy: $\mathcal{H}[G_t \mid G_{t-1}, H_{t-1}] < \mathcal{H}[G_t]$ (arrivals conditioned on removals), but cross-round $\mathbb{E}[\text{cPRA}(G_t)] > \mathbb{E}[\text{cPRA}(G_0)]$ due to retention, with racial correlation $\text{Cov}(\text{cPRA}, \text{Race}) > 0$ (e.g., regression coefficients from SRTR: $\beta_{\text{Black}} \approx 0.15-0.20$ for high-cPRA probability). Thus, assuming time-independence biases marginals, under-sampling equity-weighted $H$ for minorities, reducing planner welfare by ~10-20% in disparate access (Mohandas et al., 2022 estimates).

Outside the authors' a priori simulated pipeline—where graphs are generated homogenously without real demographic correlations—the model breaks: real UNOS data introduces persistent time-dependencies, inflating $D_{\text{KL}}$ as simulated embeddings fail to capture evolving distributions, leading to higher variance and collapse (e.g., test KL spikes 2-5x in Pan et al. analogs).

To correct, integrate Stochastic GFlowNets (Pan et al., 2023) via sub-trajectory balance, incorporating the kernel:

<p align="center">
  <img src="https://latex.codecogs.com/png.latex?F%28s%29%20%5Cpi%28a%20%5Cmid%20s%29%20P%28s%27%20%5Cmid%20s%2C%20a%29%20%3D%20F%28s%27%29%20%5Cpi_B%28%28s%2C%20a%29%20%5Cmid%20s%27%29%2C" alt="Sub-trajectory balance equation">
</p>

with losses using $\hat{P} \approx M$ (MLE or Monte Carlo samples from simulator/real traces). This aligns $P_T \to P^*$, driving $D_{\text{KL}} \to 0$, stabilizing gradients, and enabling unbiased multi-round optimization—better serving the planner by capturing full entropy for equitable, long-term welfare maximization.

## KEP Environment Simulation

The core KEP environment simulates a kidney exchange program with incompatible patient-donor pairs and optional altruistic donors. Key features include:

Pair Generation: Incompatible pairs are generated based on blood type distributions (O: 48.14%, A: 33.73%, B: 14.28%, AB: 3.85%) and cPRA levels (low: 0.05 with 70.19% prob, medium: 0.45 with 20%, high: 0.90 with 9.81%). ABO compatibility is checked, with crossmatch failures simulated for compatible pairs.
Graph Construction: A directed graph where nodes represent pairs (patient/donor blood types, cPRA) or altruists (donor blood type only). Edges represent compatibility (ABO match + negative crossmatch).
Multi-Round Simulation: Pairs arrive over rounds via Poisson process (optional; equivalent to single round for static graphs in this setup).
Cycle and Chain Detection: Identifies cycles (up to max length, e.g., 3) and chains starting from altruists (up to max length, e.g., 4).
MIP Solver: Uses integer programming to maximize transplants by selecting disjoint cycles/chains.
Environment Interface: Provides a stateful environment for stepwise actions (select cycle/chain or terminate), tracking remaining graph and matched pairs. Rewards are exponential in matched pairs upon maximal termination.
Example output for a small instance (8 pairs, no altruists):

![KEP Env Figure](img/Figure_2.png)

Nodes table with IDs, types, blood types, cPRA.
Edges table listing source-target compatibilities.
Maximum matched pairs (e.g., via MIP).
Selected cycles/chains.
Visualization shows nodes colored by patient blood type (O: red, A: cyan, B: green, AB: yellow), with directed edges for compatibilities.

## Experiment: Demonstrating Collapse in Stochastic Environments (v1)

To illustrate the problem highlighted in the mathematical analysis, I've added a preliminary experiment inspired by the stochastic chain environment from Pan et al. (2023). This serves as a proof-of-concept to show how standard GFlowNets falter in stochastic settings, while the stochastic variant holds up. It's a simple chain where you start at state 0 and can either "stop" (terminate with reward at current state) or "continue" (move forward with probability p=0.5 or stay put with 0.5). The reward function is bimodal, with peaks around N/4 and 3N/4, making it a good test for capturing multiple modes without collapse.

In this env, the standard GFlowNet treats transitions as deterministic, ignoring the probabilistic branching. As N (chain length) grows, unmodeled stochasticity injects noise into the gradients, leading to higher variance, biased marginals, and eventual mode collapse— the model favors low-entropy paths, missing the true distribution. You see the KL divergence (measuring how far the sampled distribution is from the true posterior) spike for the standard version, while the stochastic one, which explicitly includes P(s' | s, a) in the balance, keeps the divergence low and stable.

This is just v1—a quick iteration to get the ball rolling and validate the critique. I'm building this iteratively, in the spirit of fail-fast-and-iterate: prototype, test, refine, push. It's how we move at speed in a world that's not slowing down—think Musk vs. the inertia of traditional research, where velocity can be intimidating but necessary for progress. No arrogance here, just the drive to expose flaws and fix them before they cost lives in applications like KEPs. For now, this Pan-inspired toy shows why stochasticity breaks the standard approach; I'm finalizing the full KEP simulation on my local machine (with Poisson arrivals, compatibility graphs, etc.) and will push updates in the coming hours or days. Expect polished proofs, more experiments, and iterative improvements as we die and retry to get it right.

Running the code below generate the plots. Here's the performance comparison (KL divergence vs. environment size):

![Performance Comparison: Standard vs Stochastic GFlowNet](img/Figure_1.png)  <!-- Replace with actual image link or embed -->

The env is highly stochastic

In the plot, the blue line (standard) climbs sharply as N increases, showing the collapse, while orange (stochastic) rises gently, staying closer to the truth.

    Code (PyTorch-based, runs locally):

## Future Directions

The next phase involves implementing a GFlowNet model to test on the KEP environment. The goal is to extend the approach by incorporating Stochastic GFlowNets with sub-trajectory balance, evaluating whether this leads to better convergence, reduced bias, and improved handling of stochastic dynamics in KEP simulations.

## References

- Pan et al. (2023). Stochastic Generative Flow Networks.
- St-Arnaud et al. (2025). A Learning-Based Framework for KEPs.
- Bengio et al. (2021). GFlowNet Foundations.

## License

This repository, including all text, mathematical derivations, conceptual simulations, and derived works, is licensed under the Creative Commons Attribution-NonCommercial 4.0 International License (CC BY-NC 4.0). You are free to share, copy, distribute, and adapt the material for non-commercial purposes, provided you give appropriate credit to the author, provide a link to the original repository (https://github.com/yourusername/stochastic-kep-critique.git), and indicate if changes were made.

Citation Requirement: Any use of this work must include a citation to the author (please contact the repository owner for the preferred citation format) and the repository URL. For inquiries or to notify the author of use, contact the repository owner directly.

Commercial use is prohibited without explicit written permission from the author. Violations of this license will be pursued to the fullest extent permitted by law.


